== Deploying an Apache Kafka cluster with Strimzi

Let's get started by deploying an Apache Kafka cluster on OpenShift.
For this, we will use http://strimzi.io/[Strimzi], an open-source project that simplifies the process of deploying and managing Apache Kafka clusters on Kubernetes and OpenShift.

=== How Strimzi works

You can run an Apache Kafka cluster on Kubernetes, and by extension, on OpenShift, in a variety of ways, not all being equal in terms of ease of use and maintenance.

For example, you can deploy the cluster manually as a stateful set.
While this can get you past the initial hurdle of starting the cluster, soon you have to start performing more complex tasks such as changing cluster topology, modifying configuration, or administering topics.
These tasks typically require direct access to the cluster nodes and can easily become cumbersome.

==== Kubernetes Operators ====

Strimzi simplifies these tasks by using a declarative approach to cluster and topic management, implemented as https://coreos.com/operators/[Kubernetes Operators].
A Kubernetes Operator is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user.
It builds upon the basic Kubernetes resource and operator concepts but includes domain or application-specific knowledge to automate common tasks.

Instead of relying on direct deployment and management of Zookeeper and Kafka clusters, Strimzi consists of a couple of these domain-specific operators that monitor the state of the cluster, making adjustments in accordance to a desired state read from dedicated configuration resources.

For creating an Apache Kafka cluster, for instance, you need to create a resource of type `kafka` that describes the properties of the cluster, and the *_cluster operator_* will deploy the cluster for you.
If you need to change the state of the cluster, for example for changing properties or for adding new instances, all you have to do is to modify the resource and the changes will be rolled out accordingly.

Topic management works in a similar fashion: for creating and modifying topics, you only need to create and edit a set of resources of type `kafkatopic` and the *_topic operator* will do the work for you.

You will do all this as part of the first lab.

=== Connecting to Your VM

First, connect via SSH to the Vagrant VM on your machine or your remote cloud environment.
For a local Vagrant box run:

[source, sh]
$ vagrant ssh

Otherwise, run:

[source, sh]
$ ssh <YOUR SERVER IP>

If you've set up the environment using the provided Ansible scripts, switch to the `build` user:

[source, sh]
$ sudo su - build

=== Installing Strimzi

Start up OpenShift (omit the `--public-hostname` when using Vagrant):

[source, sh]
$ oc cluster up --use-existing-config --public-hostname=<YOUR SERVER IP>

Log into OpenShift:

[source, sh]
$ oc login -u developer

Download Strimzi and extract it;

[source, sh]
$ wget https://github.com/strimzi/strimzi/releases/download/0.7.0/strimzi-0.7.0.tar.gz
$ tar xzvf strimzi-0.7.0.tar.gz

Enter the `strimzi-0.7.0` directory.

[source, sh]
$ cd strimzi-0.7.0

Adjust the namespace in the installation files to `voxxed`:

[source, sh]
$ sed -i 's/namespace: .*/namespace: voxxed/' examples/install/cluster-operator/*RoleBinding*.yaml

Next, create a new project for the lab:

[source, sh]
$ oc new-project voxxed

You can see that it is empty:

[source, sh]
$ oc get pods
No resources found.

Log in as administrator to install Strimzi
(use the password provided by the lab instructors):

[source,sh]
$ oc login -u system:admin

Install the cluster operator:

[source, sh]
$ oc apply -f examples/install/cluster-operator -n voxxed

You should see a few resources being created:

[source, sh]
serviceaccount "strimzi-cluster-operator" created
clusterrole.rbac.authorization.k8s.io "strimzi-cluster-operator-namespaced" created
rolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator" created
clusterrole.rbac.authorization.k8s.io "strimzi-cluster-operator-global" created
clusterrolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator" created
clusterrole.rbac.authorization.k8s.io "strimzi-kafka-broker" created
clusterrolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator-kafka-broker-delegation" created
clusterrole.rbac.authorization.k8s.io "strimzi-entity-operator" created
rolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator-entity-operator-delegation" created
clusterrole.rbac.authorization.k8s.io "strimzi-topic-operator" created
rolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator-topic-operator-delegation" created
customresourcedefinition.apiextensions.k8s.io "kafkas.kafka.strimzi.io" created
customresourcedefinition.apiextensions.k8s.io "kafkaconnects.kafka.strimzi.io" created
customresourcedefinition.apiextensions.k8s.io "kafkaconnects2is.kafka.strimzi.io" created
customresourcedefinition.apiextensions.k8s.io "kafkatopics.kafka.strimzi.io" created
customresourcedefinition.apiextensions.k8s.io "kafkausers.kafka.strimzi.io" created
deployment.extensions "strimzi-cluster-operator" created

The service account `strimzi-cluster-operator` is granted permission to access various resources in the project.
This allows it to read the resources containing the cluster configuration that we will create later in the process.

Now, make sure that the cluster operator is deployed.

[source,sh]
$ oc get pods

The command output should be similar to:

[source,sh]
NAME                                          READY     STATUS    RESTARTS   AGE
strimzi-cluster-operator-2044197322-lzrvr   1/1       Running   0          3m

Next, install the Strimzi templates.
The Cluster Operator related templates contain predefined resources for easily deploying clusters (for Kafka Connect as well).

[source, sh]
$ oc apply -f examples/templates/cluster-operator -n voxxed
template.template.openshift.io "strimzi-connect-s2i" created
template.template.openshift.io "strimzi-connect" created
template.template.openshift.io "strimzi-ephemeral" created
template.template.openshift.io "strimzi-persistent" created

Now you can deploy a Kafka cluster.
For this lab, we will use a template (so to bootstrap only a single ZooKeeper node),
but you could create a resource file from scratch as well or use one from _examples/kafka_.
We will deploy 3 instances of Kafka broker (the default) and one instance of ZooKeeper.

TODO: decide on ZK nodes

[source, sh]
# $ oc apply -f examples/kafka/kafka-ephemeral.yaml
$ oc process strimzi-ephemeral -p ZOOKEEPER_NODE_COUNT=1 | oc apply -f -
kafka "my-cluster" created

Let's take a look at the resource we've created:

[source, sh]
$ oc describe kafka my-cluster
Name:         my-cluster
Namespace:    voxxed
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"kafka.strimzi.io/v1alpha1","kind":"Kafka","metadata":{"annotations":{},"name":"my-cluster","namespace":"voxxed"},"spec":{"entityOperator...
API Version:  kafka.strimzi.io/v1alpha1
Kind:         Kafka
Metadata:
  Cluster Name:
  Creation Timestamp:  2018-10-09T14:46:14Z
  Resource Version:    1574
  Self Link:           /apis/kafka.strimzi.io/v1alpha1/namespaces/voxxed/kafkas/my-cluster
  UID:                 12450129-cbd2-11e8-b135-96000011cbea
Spec:
  Entity Operator:
    Topic Operator:
    User Operator:
  Kafka:
    Config:
      Offsets . Topic . Replication . Factor:            3
      Transaction . State . Log . Min . Isr:             2
      Transaction . State . Log . Replication . Factor:  3
    Listeners:
      Plain:
      Tls:
    Liveness Probe:
      Initial Delay Seconds:  15
      Timeout Seconds:        5
    Metrics:
      Lowercase Output Name:  true
      Rules:
        Name:     kafka_server_$1_$2_total
        Pattern:  kafka.server<type=(.+), name=(.+)PerSec\w*><>Count
        Labels:
          Topic:  $3
        Name:     kafka_server_$1_$2_total
        Pattern:  kafka.server<type=(.+), name=(.+)PerSec\w*, topic=(.+)><>Count
    Readiness Probe:
      Initial Delay Seconds:  15
      Timeout Seconds:        5
    Replicas:                 3
    Storage:
      Type:  ephemeral
  Zookeeper:
    Liveness Probe:
      Initial Delay Seconds:  15
      Timeout Seconds:        5
    Metrics:
      Lowercase Output Name:  true
    Readiness Probe:
      Initial Delay Seconds:  15
      Timeout Seconds:        5
    Replicas:                 1
    Storage:
      Type:  ephemeral
Events:      <none>

Note how for instance the number of Kafka and ZooKeeper nodes is controlled using the `Replicas` parameters.

Visualize the running pods:

[source,sh]
$ oc get pods -w

Wait until all pods have spun up and are in `Running` status:

[source,sh]
$ oc get pods -w
NAME                                          READY     STATUS    RESTARTS   AGE
my-cluster-entity-operator-8669d89df6-g975b   3/3       Running   0          3m
my-cluster-kafka-0                            2/2       Running   0          4m
my-cluster-kafka-1                            2/2       Running   0          4m
my-cluster-kafka-2                            2/2       Running   0          4m
my-cluster-zookeeper-0                        2/2       Running   0          4m
strimzi-cluster-operator-7d8898b9b9-jfwv5     1/1       Running   0          14m

In addition to the `cluster operator` created previously, notice a few more deployments:

* the `entity operator` is now deployed as well - you can deploy it independently, but the Strimzi template deploys it out of the box; it is used to manage topics and/or users of Kafka
* one Zookeeper node
* three Kafka brokers

Also, notice that the Zookeeper ensemble and the Kafka cluster are deployed as stateful sets.

=== Monitoring with Prometheus and Grafana

By default, Strimzi provides the Kafka brokers and the Zookeeper nodes with a Prometheus JMX exporter agent which is running in order to export metrics.
These metrics can be read and processed by a Prometheus server in order to monitoring the cluster.
For building a graphical dashboard with such information, it's possible to use Grafana.

==== Prometheus

The Prometheus service pod runs with `prometheus-server` service account and it needs to have access to the API server to get the pod list and for allowing that, the following command is needed.

[source,sh]
$ export NAMESPACE=voxxed
$ oc create sa prometheus-server
$ oc adm policy add-cluster-role-to-user cluster-reader system:serviceaccount:${NAMESPACE}:prometheus-server

Finally, create the Prometheus service by running:

[source,sh]
$ oc apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/prometheus/kubernetes.yaml

==== Grafana

The Grafana server is really useful to get a visualisation of the Prometheus metrics.

To deploy Grafana on OpenShift, the following commands should be executed:

[source,sh]
$ oc apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/grafana/kubernetes.yaml

You can access the Grafana UI after running:

[source,sh]
$ oc expose svc/grafana

The hostname of the service is available in the OpenShift console, or can be retrieved via CLI:

[source,sh]
$ oc get routes grafana -o=jsonpath='{.spec.host}{"\n"}'

Note the output, which should be in the format `grafana-voxxed.<YOUR IP>.nip.io` and access the Grafana UI at that URL in your browser.
Now we can set up the Prometheus data source and the Kafka dashboard.

Access to the Grafana UI using `admin/admin` credentials.

image::grafana_login.png[grafana login]

Click on the "Add data source" button from the Grafana home in order to add Prometheus as data source.

image::grafana_home.png[grafana home]

Fill in the information about the Prometheus data source, specifying a name and "Prometheus" as type.
In the URL field, use `http://prometheus:9090` as the URL to the Prometheus server.
After "Add" is clicked, Grafana will test the connection to the data source.

image::grafana_prometheus_data_source.png[grafana prometheus data source]

From the top left menu, click on "Dashboards" and then "Import" to open the "Import Dashboard" window.
Open a browser tab and navigate to `https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.7.0/metrics/examples/grafana/kafka-dashboard.json`.
You should see JSON content as response.
Copy and paste it in the appropriate field in the form.

image::grafana_import_dashboard.png[grafana import dashboard]

After importing the dashboard, the Grafana home should show with some initial metrics about CPU and JVM memory usage.
When the Kafka cluster is used (creating topics and exchanging messages) the other metrics, like messages in and bytes in/out per topic, will be shown.

image::grafana_kafka_dashboard.png[grafana kafka dashboard]

=== Handling cluster and topics

Before starting to develop data streaming applications and running them, let's see how it's possible to handle the Kafka cluster itself and the topics.

==== Updating Kafka cluster

Starting from the current Kafka cluster with 3 brokers, we want to add two more.
In order to do that, the related `my-cluster` resource needs to be updated using the "edit" command provided by the `oc` tool.

[source,sh]
$ oc edit kafka my-cluster

It opens the default editor that we can use in order to change the value of the `kafka/replicas` field from 3 to 5.
After saving the file, the Cluster Operator detects the update and starts a two new broker Pods; it's just a simple scale-up operation.
You can see this by visualizing the pods again:

[source,sh]
$ oc get pods
NAME                                          READY     STATUS    RESTARTS   AGE
my-cluster-entity-operator-549b687c88-gb4w9   3/3       Running   0          45m
my-cluster-kafka-0                            2/2       Running   0          46m
my-cluster-kafka-1                            2/2       Running   0          46m
my-cluster-kafka-2                            2/2       Running   0          46m
my-cluster-kafka-3                            2/2       Running   0          18m
my-cluster-kafka-4                            2/2       Running   0          18m
my-cluster-zookeeper-0                        2/2       Running   0          38s
strimzi-cluster-operator-5bbcc486fc-hwswp     1/1       Running   0          47m

Notice the new pods `my-cluster-kafka-3` and `my-cluster-kafka-4`.
For the rest of the lab, we only need three Kafka brokers, so we recommend to size down the cluster to 3 nodes, by editing the `kafka/replicas` field of the `kafka` resouce again.
Also only a single ZooKeeper node (field `zookeeper/replicas) will be needed:

[source,sh]
$ oc edit kafka my-cluster

Set the `kafka-nodes` field back to 3 and check that the two additional pods have been shut down.

[source,sh]
$ oc get pods
NAME                                           READY     STATUS    RESTARTS   AGENAME                                          READY     STATUS    RESTARTS   AGE
my-cluster-entity-operator-549b687c88-gb4w9   3/3       Running   0          50m
my-cluster-kafka-0                            2/2       Running   0          51m
my-cluster-kafka-1                            2/2       Running   0          51m
my-cluster-kafka-2                            2/2       Running   0          51m
my-cluster-zookeeper-0                        2/2       Running   0          3m
strimzi-cluster-operator-5bbcc486fc-hwswp     1/1       Running   0          51m

Now we want to do something more interesting like changing a Kafka broker configuration parameter, for example the `KAFKA_DEFAULT_REPLICATION_FACTOR` one modifying its value from 1 to 2.

Before doing that let's check that the default replication factor is 1 getting the log from one of the running brokers.

[source,sh]
$ oc logs my-cluster-kafka-0 -c kafka | grep default.replication.factor
  default.replication.factor = 1
  default.replication.factor = 1

In the same way as before you can use the "edit" command and updating that value in the default editor.

[source,sh]
$ oc edit kafka my-cluster

This kind of update is much more complex because changing the Kafka broker configuration we want all the running brokers to be updated so it means that each broker needs to be restarted in order to get the new configuration.
In this case, detecting the resource update, the Cluster Operator starts a "rolling update" and each broker Pod is killed one by one and then restarted with the new configuration.

When the "rolling update" is finished we can check that the default replication factor is changed to 2.

[source,sh]
$ oc logs my-cluster-kafka-0 -c kafka | grep default.replication.factor
default.replication.factor=2
	default.replication.factor = 2
	default.replication.factor = 2

==== Handling topics

It's possible to create a topic by creating a `kafkatopic` resource from scratch, but for this lab we are going to use the related example resource file:

[source,sh]
$ oc apply -f examples/topic/kafka-topic.yaml
kafkatopic "my-topic" created

In order to check that the Topic Operator has detected the new resource and created a related topic in the Kafka cluster, we can run the official `kafka-topics.sh` tool on one of the brokers.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper localhost:2181 --describe
Topic:my-topic	PartitionCount:1	ReplicationFactor:1	Configs:segment.bytes=1073741824,retention.ms=7200000
	Topic: my-topic	Partition: 0	Leader: 1	Replicas: 1	Isr: 1

You also can examine the topic resource itself using `oc describe`:

[source,sh]
$ oc describe kafkatopic my-topic
Name:         my-topic
Namespace:    voxxed
Labels:       strimzi.io/cluster=my-cluster
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"kafka.strimzi.io/v1alpha1","kind":"KafkaTopic","metadata":{"annotations":{},"labels":{"strimzi.io/cluster":"my-cluster"},"name":"my-topi...
API Version:  kafka.strimzi.io/v1alpha1
Kind:         KafkaTopic
Metadata:
  Cluster Name:
  Creation Timestamp:  2018-10-09T14:48:53Z
  Resource Version:    2010
  Self Link:           /apis/kafka.strimzi.io/v1alpha1/namespaces/voxxed/kafkatopics/my-topic
  UID:                 7136321b-cbd2-11e8-b135-96000011cbea
Spec:
  Config:
    Retention . Ms:   7200000
    Segment . Bytes:  1073741824
  Partitions:         1
  Replicas:           1
Events:               <none>

Let's increase the partitions number now:

[source,sh]
$ oc edit kafkatopic my-topic

Set the value of `spec/partitions` to `3`.

Alternatively, you could also edit the file _examples/topic/kafka-topic.yaml_ (e.g. using _vi_) and apply it again;

[source,sh]
$ oc apply -f examples/topic/kafka-topic.yaml
kafkatopic "my-topic" configured

The Topic Operator updates the related Kafka topic accordingly.
We can check that describing the topic one more time.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper localhost:2181 --describe
Topic:my-topic	PartitionCount:3	ReplicationFactor:1	Configs:segment.bytes=1073741824,retention.ms=7200000
	Topic: my-topic	Partition: 0	Leader: 1	Replicas: 1	Isr: 1
	Topic: my-topic	Partition: 1	Leader: 2	Replicas: 2	Isr: 2
	Topic: my-topic	Partition: 2	Leader: 0	Replicas: 0	Isr: 0

Finally, a topic can be deleted like so:

[source,sh]
$ oc delete kafkatopic my-topic
kafkatopic "my-topic" deleted

The Topic Operator detects the deletion of the resource and deletes the related Kafka topic from the cluster.
We can check that listing the available topics.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper localhost:2181 --list

This time the output should be empty.

Now your Kafka cluster is running and ready to go.
Let's build some applications!
